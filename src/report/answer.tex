\documentclass[a4j,titlepage]{jarticle} 

\usepackage{graphicx} 
\usepackage{ascmac} 
\usepackage{slashbox}
\usepackage{framed}
\usepackage{eclbkbox}
\title{レコメンドシステムのアルゴリズムについて}
\author{提出者：大阪大学基礎工学部情報科学科 津久井 佑樹 \\ メールアドレス：menoshitanoaka@gmail.com}
\date{提出年月日：\today}

\begin{document}
\maketitle


\section{実装するアルゴリズムについて}
レコメンドシステムを実現するために有効と思われるアルゴリズムとしてNMF(Non-Negative Matrix Factorization)をあげる．これはある行列Rが存在したときに$R=U^TV$となるようなU，Vを求めるためのアルゴリズムである．レコメンドシステムに適用する場合，ユーザーの数をm，ユーザーが評価したアイテムの総数をn，Rを$m \times n$行列($ユーザーiのアイテムjに対する評価点をaとしたとき，r_{ij}=a$)，U,Vをそれぞれ$K \times m$行列,$K \times n$行列($\exists K << m,n$)とする．このときUの列ベクトルはそれぞれのユーザーの特徴を抽出したベクトルとなり，Vの列ベクトルはそれぞれのアイテムの特徴を抽出したベクトルとなる．また$U^TV$から生成される行列はRの要素が0だった点が別の値に置き換えられており，この値を未評価のアイテムに対する評価の予測値として扱うことができる．これを用いてユーザーに対してレコメンドを行う．

行列RをUとVに分解する際には確率的勾配降下法を用いる．このアルゴリズムは初めにランダムに生成したUとVに対して，Rと$U^TV$の二乗誤差が最小になるように修正を繰り返すものである．今回実装する際には純粋な二乗誤差ではなく
\[f(U,V)=\frac{1}{2}\sum_{(i,j) \in D} (r_{ij}-{\bf u}_{i}^T{\bf v}_{j})^2 + \frac{\lambda}{2}(||U||_{F}^2+||V||_{F}^2)\]
を最小にするUとVを求める．$(i,j) \in D$は評価済みのユーザーとアイテムの集合を表し$\lambda$は正則化パラメータ，$(||U||_{F}^2+||V||_{F}^2)$は正則化項である．U,Vは以下の式
\[{\bf u}_{i} = {\bf u}_{i} - \eta\{-(r_{ij}-{\bf u}_{i}^T{\bf v}_{j}){\bf v}_{j} + \lambda {\bf u}_{i}\}\]
\[{\bf v}_{j} = {\bf u}_{i} - \eta\{-(r_{ij}-{\bf u}_{i}^T{\bf v}_{j}){\bf u}_{i} + \lambda {\bf v}_{j}\}\]
で更新する．$\eta$は学習率を表し，
\[\eta = \frac{1}{\alpha(t+\beta)}\]
を用いて更新する．tは反復回数を表し，$\alpha$,$\beta$はパラメータである．これらの更新は規定回数を上限として更新前と更新後の$f(U,V)$の差が一定値一定値以上の際に繰り返し行う．UとVがそれぞれ求まったら以下の式\[\max_{(i,j)\in E}{\bf u}_{i}^T{\bf v}_{j}\]
からユーザーiの未評価アイテムjの予測値の最高値が得る．このjをiに対してレコメンドする．なお，$(i,j) \in E$は未評価のユーザーとアイテムの集合を表す．

今回実装する際に各パラメータは$K=30, \lambda=0.1, \alpha=0.05, \beta=500$とし，更新の上限回数を5000，更新前と更新後の$f(U,V)$の差が0.0001以上の場合に更新を繰り返すものとした．
また，以上で用いた全式は\cite{kamishima}を参考にした．

\section{実装した手法の改善点}
第一の改善点として実装した手法ではユーザーの評価に関する時間的な特性を考慮に入れていない点が挙げられる．与えられたデータセットにはユーザーが評価を行った際のタイムスタンプが含まれているが，それが結果の導出過程で一切触れられていないため時間の経過等がユーザーの評価に与える影響を反映できていない．また，結果の導出に関わらないため結果に対して実際にそれが影響を与えているかの判断も不可能である．\cite{a}によるとユーザーが初めて評価をしてから時間が経過するごとに厳しく評価するようになる傾向があると述べられているため，それを手法に取り入れて実際にその傾向を示すか確かめる必要があると思われる．

第二に処理時間が長いという点が挙げられる．今回実装したプログラムはPython3.51で記述しており，私の実行環境(Windows10Pro，Intel(R)Core(TM)i7-4500U CPU @1.80GHZ 2.39GHz， RAM4.00GB)において与えられたデータセットから学習を行い，確率的勾配降下法を用いての更新を5000回した場合には約16時間ほど必要とした．これは後述の改善点にも影響を与える問題のため，C言語等の処理が速い言語に書き換える，アルゴリズムを改善する等の必要がある．また，$\eta$等のパラメータは結果の収束速度に影響を及ぼすためこれらを修正することによる改善も可能であると考えられる．

第三にデータとして既存のユーザーやアイテムに対するレコメンドは行えるが新規の物に対してそれを行えないという点が挙げられる．実装したアルゴリズムでは既存のユーザー，アイテム，評価点からなる行列をユーザーとアイテムそれぞれの特徴ベクトルに分解することでレコメンドを行っている．対して，新規のユーザーやアイテムに関しては特徴ベクトルが不明なため元のデータに加えてから再計算を行わない限りレコメンドできない．しかし，前途のように一度の学習に非常に時間がかかるため計算し直す際のコストが高い．

第四に導出結果の妥当性が示されていない．実装したプログラムでは$\frac{1}{2}\sum_{(i,j) \in D}\frac{1}{2}(r_{ij}-{\bf u}_{i}^T{\bf v}_{j})^2$の値を求めているが，これだけでは妥当性を示すものとして不十分である．改善策のひとつとして交差検証が挙げられるが前途のように一度の処理に時間がかかるため交差検証の結果の出力にはさらなる時間がかかることが予想される．

第五に1人のユーザーに対して1つのアイテムしかレコメンドしていない点が挙げられる．レコメンドされたアイテムに対するユーザーの反応としてユーザーがアイテムに興味を持ち実際に楽しめた場合，ユーザーが興味を持つも楽しめなかった場合，ユーザーが興味を持たなかった場合が挙げられる．一般にレコメンド数を増やすと，ユーザーがアイテムに興味を持ち実際に楽しめる場合が増えると予想されるが同時にその他の場合も増えると予想される．
それぞれに場合についてユーザーがシステムについてどれほどの好印象を抱いたか，あるいは悪印象を抱いたかのデータを収集することによってレコメンド数として妥当な値の算出ができるものと思われる．例えば，ユーザーがレコメンドされたアイテムに興味を示さなかった場合のシステムに対する悪印象が少ないと予想される場合は，興味を示さないようなアイテムが多く含まれると予想されてもレコメンド数を増やすべきである．

\newpage
\begin{thebibliography}{num=9}
\bibitem{kamishima} \begin{verbatim}
推薦システム / 上嶌 敏弘
''http://www.kamishima.net/archive/recsys.pdf''
2016年7月10日にアクセス
\end{verbatim}
\bibitem{a} \begin{verbatim}
Winning the Netflix Prize: A Summary / Edwin Chen
''http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/''
2016年7月10日にアクセス
\end{verbatim}
\bibitem{b} \begin{verbatim}
Introduction to Algorithms for Behavior Based Recommendation / Kimikazu Kato
''http://www.slideshare.net/hamukazu/introduction-to-behavior-based-
recommendation-system''
2016年7月10日にアクセス
\end{verbatim}
\bibitem{c} \begin{verbatim}
Collaborative Filtering for Implicit Feedback Datasets / Yifan Hu，Yehuda Koren，Chris Volinsky
''http://yifanhu.net/PUB/cf.pdf''
2016年7月10日にアクセス
\end{verbatim}
\end{thebibliography}
\end{document}
